{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1844a76b",
   "metadata": {},
   "source": [
    "# å®‰è£…ç¨‹åº\n",
    "\n",
    "å…‹éš†ä»“åº“ï¼Œå®‰è£…ä¾èµ– & æ£€æŸ¥OneFlow & GPU ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b8d4696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ğŸš€ v1.0.0-5-g0e573b7 Python-3.8.13 oneflow-0.8.1.dev20221021+cu112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (64 CPUs, 503.6 GB RAM, 600.9/878.1 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import oneflow as flow\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21d547",
   "metadata": {},
   "source": [
    "# 1. æ¨ç†\n",
    "\n",
    "`detect.py` å¯åœ¨ä¸åŒæ¥æºä¸Šè¿è¡ŒYOLOv5æ¨ç†, æ¨¡å‹å°†è‡ªåŠ¨ä¸‹è½½ä»[æœ€è¿‘çš„ YOLOv5 release](https://github.com/Oneflow-Inc/one-yolov5/releases), æ¨ç†çš„ç»“æœä¿å­˜åœ¨ç›®å½• `runs/detect` ã€‚ä¸‹é¢æ˜¯ä½¿ç”¨çš„æ¨ç†æ¡ˆä¾‹:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image \n",
    "                          vid.mp4  # video\n",
    "                          path/  # directory\n",
    "                          path/*.jpg  # glob\n",
    "                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85f151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded library: /lib/x86_64-linux-gnu/libibverbs.so.1\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5 ğŸš€ v1.0.0-5-g0e573b7 Python-3.8.13 oneflow-0.8.1.dev20221021+cu112 \n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7225885 parameters, 229245 gradients\n",
      "detect.py:159: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
      "image 1/2 /home/fengwen/one-yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, Done. (0.011s)\n",
      "detect.py:159: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
      "image 2/2 /home/fengwen/one-yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, Done. (0.007s)\n",
      "0.7ms pre-process, 9.3ms inference, 2.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights yolov5s --img 640 --conf 0.25 --source data/images\n",
    "#display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160aced6",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e357730",
   "metadata": {},
   "source": [
    "# 2. éªŒè¯\n",
    "åœ¨[COCO](https://cocodataset.org/#home)æ•°æ®é›†ä¸Švalæˆ–test-devéªŒè¯æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æ¨¡å‹ä»æœ€æ–°çš„[one-yolov5 releases](https://github.com/Oneflow-Inc/one-yolov5/releases) è‡ªåŠ¨ä¸‹è½½ã€‚å¦‚æœè¦æŒ‰ç±»æ˜¾ç¤ºç»“æœï¼Œè¯·ä½¿ç”¨--verboseæ ‡æ˜ã€‚\n",
    "æ³¨æ„ï¼šè¯·æ³¨æ„ï¼Œç”±äºmAPè®¡ç®—ä¸­çš„ç»†å¾®å·®å¼‚ `pycocotools` æŒ‡æ ‡å¯èƒ½æ¯”ç­‰æ•ˆå›å½’æŒ‡æ ‡å¥½~1% ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚(\n",
    "Note that `pycocotools` metrics may be ~1% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be950d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded library: /lib/x86_64-linux-gnu/libibverbs.so.1\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/coco.yaml, weights=['yolov5s'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
      "YOLOv5 ğŸš€ v1.0.0-5-g0e573b7 Python-3.8.13 oneflow-0.8.1.dev20221021+cu112 \n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7225885 parameters, 229245 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/data/dataset/fengwen/coco/val2017.cache' images and labels... 49\u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 1.260s exceeded\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       5000      36335       0.67      0.518      0.563      0.369\n",
      "Speed: 0.1ms pre-process, 0.9ms inference, 2.1ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5s_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=5.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=47.53s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=10.95s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.372\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.568\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.400\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.210\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.514\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.563\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.376\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.621\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.720\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Validate One-YOLOv5s on COCO val\n",
    "!python val.py --weights yolov5s --data data/coco.yaml --img 640 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad7442",
   "metadata": {},
   "source": [
    "# 3. è®­ç»ƒ\n",
    "\n",
    "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png\"/></a></p>\n",
    "é€šè¿‡ä½¿ç”¨pipå®‰è£…çš„ `roboflow`åŒ…ï¼Œä»æ¨æ–­æ¡ä»¶ä¸­é‡‡æ ·å›¾åƒæ¥å…³é—­ä¸»åŠ¨å­¦ä¹ å¾ªç¯\n",
    "<br><br>\n",
    "\n",
    "è®­ç»ƒ YOLOv5s æ¨¡å‹åœ¨ [COCO128](https://www.kaggle.com/ultralytics/coco128) æ•°æ®é›†ä¸Š,è¡¨ç°åœ¨ä¸‹é¢æŒ‡ä»¤ä¸­ä¸º: `--data coco128.yaml`, ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ `--weights yolov5s`, æˆ–ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ `--weights '' --cfg yolov5s.yaml`.\n",
    "\n",
    "- **é¢„è®­ç»ƒæƒé‡ [Models](https://github.com/Oneflow-Inc/one-yolov5/tree/master/models)** èƒ½å¤Ÿè¢«è‡ªåŠ¨ä¸‹è½½ä» [latest one-yolov5 release](https://github.com/Oneflow-Inc/one-yolov5/releases)\n",
    "- **[Datasets](https://github.com/Oneflow-Inc/one-yolov5/tree/main/data)** èƒ½å¤Ÿè¢«è‡ªåŠ¨ä¸‹è½½æ¯”å¦‚ [COCO](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/Oneflow-Inc/one-yolov5/blob/master/data/SKU-110K.yaml).\n",
    "- **è®­ç»ƒç»“æœ** ä¿å­˜åœ¨ `runs/train/` ç›®å½•ä¸‹å¢åŠ è¿è¡Œç›®å½•, ä¾‹å¦‚: `runs/train/exp2`, `runs/train/exp3` ç­‰ã€‚ã€‚ã€‚\n",
    "<br><br>\n",
    "\n",
    " **A Mosaic Dataloader** ï¼šéšæœºå°†4å¼ å›¾åƒç»„åˆæˆ1å¼ å›¾è®­ç»ƒæ–¹æ³•ã€‚\n",
    "\n",
    "\n",
    "## é€šè¿‡ Roboflow è‡ªå®šä¹‰æ•°é›†è®­ ğŸŒŸ NEW\n",
    "[Roboflow](https://roboflow.com/?ref=ultralytics)  ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´åŠ å®¹æ˜“ `ç»„ç»‡`,`æ ‡è®°`,`å‡†å¤‡` ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ã€‚\n",
    "Roboflow è¿˜å¯ä»¥å¾ˆå®¹æ˜“çš„å»ºç«‹ä¸€ä¸ªä¸»åŠ¨å­¦ä¹ çš„æµç¨‹ï¼Œå¯ä»¥é€šè¿‡å›¢é˜Ÿçš„å½¢å¼åœ¨åä½œåˆ¶ä½œæ•°æ®é›†ï¼Œå¹¶å¯ä»¥é€šè¿‡pip `roboflow` åŒ…å¯ä»¥ç›´æ¥é›†æˆåˆ°æ¨¡å‹æ„å»ºå·¥ä½œæµä¸­ã€‚\n",
    "\n",
    "- è‡ªå®šä¹‰è®­ç»ƒä¾‹å­: [https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics)\n",
    "- åœ¨Notebookä¸­è‡ªå®šä¹‰è®­ç»ƒ: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)\n",
    "<br>\n",
    "\n",
    "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"480\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a275ad4b4ac20cd2e21a_roboflow-annotate.gif\"/></a></p>\n",
    "å¿«é€Ÿåœ¨å›¾ç‰‡ä¸Šæ·»åŠ æ ‡ç­¾ï¼ˆå…¶ä¸­åŒ…æ‹¬æ¨¡å‹è¾…åŠ©æ ‡ç­¾ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42214a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded library: /lib/x86_64-linux-gnu/libibverbs.so.1\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s, cfg=, data=data/coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "fatal: ambiguous argument 'main..origin/master': unknown revision or path not in the working tree.\n",
      "Use '--' to separate paths from revisions, like this:\n",
      "'git <command> [<revision>...] -- [<file>...]'\n",
      "Command 'git rev-list main..origin/master --count' returned non-zero exit status 128.\n",
      "YOLOv5 ğŸš€ v1.0.0-5-g0e573b7 Python-3.8.13 oneflow-0.8.1.dev20221021+cu112 \n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ğŸš€ runs in Weights & Biases\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ğŸš€ runs in ClearML\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  oneflow.nn.modules.upsampling.Upsample  [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  oneflow.nn.modules.upsampling.Upsample  [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients\n",
      "\n",
      "Transferred 349/349 items from yolov5s\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/data/dataset/fengwen/coco128/labels/train2017.cache' images an\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 1607.5\u001b[0m\n",
      "flow.env.get_world_size() >>>>>  1 flow.env.get_rank() >>>>> 0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/data/dataset/fengwen/coco128/labels/train2017.cache' images and \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 1567.19i\u001b[0m\n",
      "Plotting labels to runs/train/exp4/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp4\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "       0/0   0.04458   0.06735   0.01981       256       640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.739      0.627      0.726      0.486\n",
      "\n",
      "1 epochs completed in 0.001 hours\n",
      "Optimizer stripped from runs/train/exp4/weights/last, 0.0MB\n",
      "Optimizer stripped from runs/train/exp4/weights/best, 0.0MB\n",
      "\n",
      "Validating runs/train/exp4/weights/best...\n",
      "runs/train/exp4/weights/best\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7225885 parameters, 229245 gradients\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929       0.74      0.625      0.726      0.486\n",
      "              person        128        254      0.852      0.717      0.807      0.532\n",
      "             bicycle        128          6      0.701      0.402      0.695      0.385\n",
      "                 car        128         46      0.806      0.391      0.563      0.244\n",
      "          motorcycle        128          5      0.829        0.8      0.872      0.721\n",
      "            airplane        128          6      0.938          1      0.995      0.719\n",
      "                 bus        128          7      0.645      0.714      0.762      0.658\n",
      "               train        128          3      0.655      0.667      0.789      0.493\n",
      "               truck        128         12       0.64      0.417      0.485      0.251\n",
      "                boat        128          6      0.886      0.333      0.446      0.191\n",
      "       traffic light        128         14      0.503      0.145      0.365      0.226\n",
      "           stop sign        128          2      0.809          1      0.995      0.747\n",
      "               bench        128          9      0.668      0.444      0.588      0.237\n",
      "                bird        128         16      0.917          1      0.991      0.641\n",
      "                 cat        128          4      0.875          1      0.995      0.822\n",
      "                 dog        128          9      0.843      0.667      0.825      0.599\n",
      "               horse        128          2      0.756          1      0.995      0.672\n",
      "            elephant        128         17      0.962      0.882      0.932      0.722\n",
      "                bear        128          1      0.651          1      0.995      0.895\n",
      "               zebra        128          4      0.865          1      0.995      0.905\n",
      "             giraffe        128          9      0.896      0.957      0.984      0.717\n",
      "            backpack        128          6      0.823        0.5      0.708      0.322\n",
      "            umbrella        128         18       0.72      0.572      0.822       0.46\n",
      "             handbag        128         19      0.405      0.105      0.297      0.153\n",
      "                 tie        128          7      0.835      0.571      0.699      0.482\n",
      "            suitcase        128          4      0.633          1      0.995      0.596\n",
      "             frisbee        128          5      0.713        0.8       0.76       0.68\n",
      "                skis        128          1      0.671          1      0.995      0.303\n",
      "           snowboard        128          7      0.802       0.58      0.836      0.499\n",
      "         sports ball        128          6      0.641      0.667      0.602      0.309\n",
      "                kite        128         10      0.693      0.456      0.585      0.219\n",
      "        baseball bat        128          4      0.397        0.5      0.425       0.13\n",
      "      baseball glove        128          7      0.536      0.429      0.452      0.322\n",
      "          skateboard        128          5      0.697        0.4      0.671      0.497\n",
      "       tennis racket        128          7      0.458      0.429      0.544      0.289\n",
      "              bottle        128         18      0.611      0.437      0.562      0.278\n",
      "          wine glass        128         16      0.716      0.875      0.901      0.546\n",
      "                 cup        128         36      0.815      0.556      0.794      0.531\n",
      "                fork        128          6      0.954      0.333      0.411      0.295\n",
      "               knife        128         16      0.767      0.688      0.666      0.392\n",
      "               spoon        128         22      0.828      0.439      0.632      0.379\n",
      "                bowl        128         28      0.887      0.563      0.708      0.489\n",
      "              banana        128          1      0.816          1      0.995      0.399\n",
      "            sandwich        128          2          1          0      0.595      0.516\n",
      "              orange        128          4       0.96          1      0.995      0.691\n",
      "            broccoli        128         11      0.528      0.364      0.411      0.326\n",
      "              carrot        128         24      0.693      0.542      0.711      0.467\n",
      "             hot dog        128          2      0.594          1      0.828      0.745\n",
      "               pizza        128          5      0.827      0.959      0.962      0.693\n",
      "               donut        128         14      0.674          1      0.981      0.872\n",
      "                cake        128          4      0.735          1      0.995      0.797\n",
      "               chair        128         35      0.565      0.657      0.567      0.278\n",
      "               couch        128          6          1      0.621      0.828      0.529\n",
      "        potted plant        128         14      0.782      0.714       0.82      0.493\n",
      "                 bed        128          3          1          0      0.696      0.406\n",
      "        dining table        128         13      0.833      0.384      0.559      0.368\n",
      "              toilet        128          2      0.839          1      0.995      0.846\n",
      "                  tv        128          2      0.638          1      0.995      0.796\n",
      "              laptop        128          3          1          0      0.775      0.392\n",
      "               mouse        128          2          1          0     0.0813     0.0407\n",
      "              remote        128          8      0.956      0.625      0.629      0.518\n",
      "          cell phone        128          8      0.599      0.374      0.429      0.221\n",
      "           microwave        128          3      0.767          1      0.995      0.669\n",
      "                oven        128          5      0.352        0.4      0.411      0.287\n",
      "                sink        128          6      0.317      0.167      0.269       0.19\n",
      "        refrigerator        128          5      0.621        0.8      0.806       0.53\n",
      "                book        128         29      0.464      0.207       0.31      0.157\n",
      "               clock        128          9      0.679      0.778      0.885      0.726\n",
      "                vase        128          2      0.425          1      0.995      0.895\n",
      "            scissors        128          1          1          0      0.199     0.0398\n",
      "          teddy bear        128         21      0.761      0.456      0.764      0.454\n",
      "          toothbrush        128          5      0.815      0.889      0.928      0.602\n",
      "Results saved to \u001b[1mruns/train/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### Train YOLOv5s on COCO128 for 3 epochs\n",
    "!python train.py --img 640 --batch 16 --epochs 1 --data data/coco128.yaml --weights yolov5s --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eede3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select YOLOv5 ğŸš€ logger {run: 'auto'}\n",
    "logger = 'TensorBoard' #@param ['TensorBoard', 'Comet', 'ClearML']\n",
    "if logger == 'TensorBoard':\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir runs/train\n",
    "elif logger == 'Comet':\n",
    "  %pip install -q comet_ml\n",
    "  import comet_ml; comet_ml.init()\n",
    "elif logger == 'ClearML':\n",
    "  %pip install -q clearml && clearml-init"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cce302673dfae556bef7680a7d0b4663031f31725223d1d27ce80f36e27e2261"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
